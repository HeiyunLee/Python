{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fad5a4f",
   "metadata": {},
   "source": [
    "## 19. 세상에 없는 얼굴 GAN, 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb5184",
   "metadata": {},
   "source": [
    "### 1. 가짜 제조 공정, 생성자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39a4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30674a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b428bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자 모델 만들기\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim = 100, activation=LeakyReLU(0.2)))\n",
    "# 128 : 임의로 정한 노드 수(128이 아니어도 되며, 충분한 노드 마련해주기)\n",
    "# input_dim = 100 : 100차원 크기의 랜덤 벡터를 준비해 집어넣으라는 의미  ----> 어디에?\n",
    "# 7*7 : 이미지의 최초 크기, 이유: \n",
    "\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7,7,128)))\n",
    "generator.add(UpSampling2D())\n",
    "# UpSampling2D : 이미지의 크기를 두 배씩 늘려준다.\n",
    "# 7*7 레이어를 지나며 이미지 크기가 14*14가 되고, \n",
    "\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "# 두 번째 UpSampling2D를 지나면 이미지 크기가 28*28이 된다.\n",
    "# 작은 크기의 이미지를 점점 늘려 가면서 컨볼루션 층을 지나게 하는 것이 DCGAN의 특징\n",
    "\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db9b4f",
   "metadata": {},
   "source": [
    "### 2. 진위를 가려내는 장치, 판별자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ac01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판별자 모델 만들기\n",
    "# 모델 이름을 discriminator로 정하고 Sequential() 함수로 호출\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding='same'))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fba91a",
   "metadata": {},
   "source": [
    "### 3. 적대적 신경망 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f582ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ginput = Input(shape=(100,))\n",
    "dis_output = discriminator(generator(ginput)) \n",
    "gan = Model(ginput, dis_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301185b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 함수를 선언합니다.\n",
    "def gan_train(epoch, batch_size, saving_interval): # 세 가지 변수 지정\n",
    "\n",
    "# MNIST 데이터 불러오기\n",
    "    # MNIST 데이터를 다시 불러와 이용합니다. 단, 테스트 과정은 필요 없고\n",
    "    # 이미지만 사용할 것이기 때문에 X_train만 호출합니다.\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # 가로 28픽셀, 세로 28픽셀이고 흑백이므로 1을 설정합니다.\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "    # 0~255 사이 픽셀 값에서 127.5를 뺀 후 127.5로 나누면 -1~1 사이 값으로 바뀝니다.\n",
    "    X_train = (X_train - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b42c0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16884\\2343806123.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "true = np.ones((batch_size, 1))\n",
    "idx = np.random.randint(0, X_train.shape[0], batch_size) \n",
    "imgs = X_train[idx]\n",
    "d_loss_real = discriminator.train_on_batch(imgs, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.zeros((batch_size, 1)) \n",
    "noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "gen_imgs = generator.predict(noise)\n",
    "d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8b4bfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_loss_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16884\\1412636070.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# d_loss_real, d_loss_fake 값을 더해 둘로 나눈 평균이 바로 판별자의 오차\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd_loss_real' is not defined"
     ]
    }
   ],
   "source": [
    "# d_loss_real, d_loss_fake 값을 더해 둘로 나눈 평균이 바로 판별자의 오차\n",
    "d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss = gan.train_on_batch(noise, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e7a2a",
   "metadata": {},
   "source": [
    "### GAN 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2628f467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (7345740.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2544\\7345740.py\"\u001b[1;36m, line \u001b[1;32m78\u001b[0m\n\u001b[1;33m    noise = np.random.normal(0, 1, (25, 100))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape,Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 생성자 모델을 만듭니다.\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))\n",
    "\n",
    "# 판별자 모델을 만듭니다.\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator.trainable = False\n",
    "\n",
    "# 생성자와 판별자 모델을 연결시키는 gan 모델을 만듭니다.\n",
    "ginput = Input(shape=(100,))\n",
    "dis_output = discriminator(generator(ginput))\n",
    "gan = Model(ginput, dis_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()\n",
    "\n",
    "# 신경망을 실행시키는 함수를 만듭니다.\n",
    "def gan_train(epoch, batch_size, saving_interval):\n",
    "\n",
    "# MNIST 데이터 불러오기\n",
    "    # 앞서 불러온 MNIST를 다시 이용합니다. 테스트 과정은 필요 없고\n",
    "    # 이미지만 사용할 것이기 때문에 X_train만 호출합니다.\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "    # 127.5를 뺀 후 127.5로 나누어서 -1~1 사이의 값으로 바꿉니다.\n",
    "    X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "    true = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # 실제 데이터를 판별자에 입력하는 부분입니다.\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, true)\n",
    "\n",
    "        # 가상 이미지를 판별자에 입력하는 부분입니다.\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "        # 판별자와 생성자의 오차를 계산합니다.\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        g_loss = gan.train_on_batch(noise, true)\n",
    "\n",
    "        print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss)\n",
    "\n",
    "        # 중간 과정을 이미지로 저장하는 부분입니다. 정해진 인터벌만큼 학습되면\n",
    "        # 그때 만든 이미지를 gan_images 폴더에 저장하라는 의미입니다.\n",
    "        # 이 코드는 이 장의 주된 목표와는 관계가 없어서 소스 코드만 소개합니다.\n",
    "        if i % saving_interval == 0:\n",
    "        # r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        count = 0\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
    "                axs[j, k].axis('off')\n",
    "                count += 1\n",
    "        fig.savefig(\"gan_images/gan_mnist_%d.png\" % i)\n",
    "\n",
    "# 2000번 반복되고(+1을 하는 것에 주의),\n",
    "# 배치 크기는 32, 200번마다 결과가 저장됩니다.\n",
    "gan_train(2001, 32, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb822f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
