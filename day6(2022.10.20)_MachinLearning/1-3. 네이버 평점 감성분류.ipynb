{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89324145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80326950",
   "metadata": {},
   "source": [
    "### 1. 목적\n",
    "- 네이버 영화리뷰 데이터셋을 이용해서 긍정/부정 분류 모델을 만들어보자\n",
    "- TF-IDF를 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01270f48",
   "metadata": {},
   "source": [
    "### 2. 데이터 수집\n",
    "- 네이버 영화 리뷰 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453e77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ratings.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1c74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        200000 non-null  int64 \n",
      " 1   document  199992 non-null  object\n",
      " 2   label     200000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c10ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 199992 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        199992 non-null  int64 \n",
      " 1   document  199992 non-null  object\n",
      " 2   label     199992 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5132c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['document']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5871ffd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n",
       "1         디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...\n",
       "2                      폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n",
       "3         와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...\n",
       "4                               안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n",
       "                                ...                        \n",
       "199995                                       포켓 몬스터 짜가 ㅡㅡ;;\n",
       "199996                                                쓰.레.기\n",
       "199997                    완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.\n",
       "199998                  왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ\n",
       "199999                                      포풍저그가나가신다영차영차영차\n",
       "Name: document, Length: 199992, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbadb214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "199995    0\n",
       "199996    0\n",
       "199997    0\n",
       "199998    0\n",
       "199999    0\n",
       "Name: label, Length: 199992, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14504cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                   test_size=0.3,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e641c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66599                                      선리플 후감상10자압박뒤집어져\n",
       "38068                                  ㅋㅋ 넘재밌어 나쁜아이없다~~~~~~\n",
       "96045                 1편보다 크게 못할것까진 없었는데..3편은 더 빵 터지길 기대해봄~\n",
       "40100                                                 깜동 ..\n",
       "161918                  대단하다. 어떻게 유명영화를 그대로 베껴서 만드냐ㅋㅋ 중국짱짱맨\n",
       "                                ...                        \n",
       "176970                          엄청지루....그냥 그저 그렇게 흘러가다 끝나네요\n",
       "117956                                   배우들의 카리스마가 너무 부족하다\n",
       "173692    도저히 몰입할 수없는 캐릭터 설득력없는 스토리 쓸데없는 복선과 배경설정 지나치게 가...\n",
       "43567                                         영상미는 정말 ㅋㅋㅋㅋㅋ\n",
       "199348                  시간 아까움1시간이상을 1명에 좁은 공간이라 보는 내가 답답함.\n",
       "Name: document, Length: 139994, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee40aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19029144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '학교', '날']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 토큰화\n",
    "okt.nouns('오늘은 학교에 가는 날이에요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1e5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장이 들어왔을 때 명사만 토큰화 시키는 함수를 선언\n",
    "def Tokenizer(text) :\n",
    "    return okt.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb8eed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'선': 5,\n",
       " '리플': 3,\n",
       " '후': 10,\n",
       " '감상': 0,\n",
       " '자': 7,\n",
       " '압박': 6,\n",
       " '편': 9,\n",
       " '크게': 8,\n",
       " '더': 2,\n",
       " '빵': 4,\n",
       " '기대': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 방식 활용\n",
    "# tokenizer : 토큰 단위를 지정, 디폴트는 word\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer = Tokenizer)\n",
    "\n",
    "# 3개 행에 대해서 진행해봄\n",
    "tfidf.fit(x_train[:3])\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b27e25",
   "metadata": {},
   "source": [
    "### pos tagging\n",
    "- 문장을 토큰화 한 후 쪼개진 형태소에 품사를 부여하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab4a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글에서는 형용사와 동사에 감정이 많이 들어가있음\n",
    "data = \"오늘 날씨가 추워서 기분이 좋지않다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17daccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '날씨', '가', '추워서', '기분', '이', '좋지', '않다', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(data) # 형태소로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc607342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('날씨', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('추워서', 'Adjective'),\n",
       " ('기분', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('좋지', 'Adjective'),\n",
       " ('않다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(data) # 형태소 분리 + 품사 태그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b11a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adjective': '형용사',\n",
       " 'Adverb': '부사',\n",
       " 'Alpha': '알파벳',\n",
       " 'Conjunction': '접속사',\n",
       " 'Determiner': '관형사',\n",
       " 'Eomi': '어미',\n",
       " 'Exclamation': '감탄사',\n",
       " 'Foreign': '외국어, 한자 및 기타기호',\n",
       " 'Hashtag': '트위터 해쉬태그',\n",
       " 'Josa': '조사',\n",
       " 'KoreanParticle': '(ex: ㅋㅋ)',\n",
       " 'Noun': '명사',\n",
       " 'Number': '숫자',\n",
       " 'PreEomi': '선어말어미',\n",
       " 'Punctuation': '구두점',\n",
       " 'ScreenName': '트위터 아이디',\n",
       " 'Suffix': '접미사',\n",
       " 'Unknown': '미등록어',\n",
       " 'Verb': '동사'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ad3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morph</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Verb</th>\n",
       "      <td>않다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>오늘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>날씨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noun</th>\n",
       "      <td>기분</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adjective</th>\n",
       "      <td>추워서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adjective</th>\n",
       "      <td>좋지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          morph\n",
       "tag            \n",
       "Verb         않다\n",
       "Noun         오늘\n",
       "Noun         날씨\n",
       "Noun         기분\n",
       "Adjective   추워서\n",
       "Adjective    좋지"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(okt.pos(data), columns= ['morph','tag'])\n",
    "df.set_index('tag', inplace=True) # tag를 인덱스로 지정\n",
    "df.loc[['Verb','Noun','Adjective']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b36920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer1(text):\n",
    "    # 텍스트가 들어오면 데이터프레임으로 변경 후 품사를 인덱스로 설정\n",
    "    df = pd.DataFrame(okt.pos(text), columns=['morph','tag'])\n",
    "    df.set_index('tag', inplace=True)\n",
    "    \n",
    "    # 동사 명사 형용사일 때 그 값을 반환\n",
    "    if ('Verb' in df.index)|('Adjective' in df.index) | ('Noun' in df.index):\n",
    "        labels = ['Verb','Adjective','Noun']\n",
    "        # intersection() : 괄호 안의 라벨들의 교집합까지 포함해서 배열로 변환\n",
    "        return df.loc[df.index.intersection(labels)]['morph'].values\n",
    "    # 형용사 동사 명사 아니면 빈 배열로 반환\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e087a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['오늘', '날씨', '기분', '추워서', '좋지', '않다'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93f0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minzy/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'선': 8,\n",
       " '리플': 6,\n",
       " '후': 19,\n",
       " '감상': 0,\n",
       " '자': 12,\n",
       " '압박': 9,\n",
       " '뒤집어져': 5,\n",
       " '넘': 3,\n",
       " '재밌어': 13,\n",
       " '나쁜아이': 2,\n",
       " '없다': 10,\n",
       " '편': 16,\n",
       " '크게': 14,\n",
       " '더': 4,\n",
       " '빵': 7,\n",
       " '기대': 1,\n",
       " '할것까진': 17,\n",
       " '터지길': 15,\n",
       " '해봄': 18,\n",
       " '없었는데': 11}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=Tokenizer1)\n",
    "tfidf.fit(x_train[:3])\n",
    "tfidf.vocabulary_ # 동사, 형용사, 명사 추출한 단어모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23b27843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b07ee8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minzy/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function Tokenizer1 at 0x2a1f4f040>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(tokenizer=Tokenizer1)\n",
    "tfidf1.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bc3d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4188"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동사, 형용사, 일반명사만 추출하여 만들어진 단어모음의 개수\n",
    "len(tfidf1.vocabulary_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afb6e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 단어를 기계가 이해할 수 있도록 벡터형태로 변환\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb338fa3",
   "metadata": {},
   "source": [
    "### 모델적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4586e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64276081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.529"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f44f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344e9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
